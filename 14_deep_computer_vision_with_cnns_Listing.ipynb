{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chapter 14 – Deep Computer Vision Using Convolutional Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20 and TensorFlow ≥2.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Is this notebook running on Colab or Kaggle?\n",
    "IS_COLAB = \"google.colab\" in sys.modules\n",
    "IS_KAGGLE = \"kaggle_secrets\" in sys.modules\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"No GPU was detected. CNNs can be very slow without a GPU.\")\n",
    "    if IS_COLAB:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
    "    if IS_KAGGLE:\n",
    "        print(\"Go to Settings > Accelerator and select GPU.\")\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"cnn\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple utility functions to plot grayscale and RGB images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image):\n",
    "    plt.imshow(image, cmap=\"gray\", interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "def plot_color_image(image):\n",
    "    plt.imshow(image, interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is a Convolution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_sample_image\n",
    "\n",
    "# Load sample images\n",
    "china = load_sample_image(\"china.jpg\") / 255\n",
    "flower = load_sample_image(\"flower.jpg\") / 255\n",
    "images = np.array([china, flower])\n",
    "batch_size, height, width, channels = images.shape\n",
    "\n",
    "# Create 2 filters\n",
    "filters = np.zeros(shape=(7, 7, channels, 2), dtype=np.float32)\n",
    "filters[:, 3, :, 0] = 1  # vertical line\n",
    "filters[3, :, :, 1] = 1  # horizontal line\n",
    "\n",
    "outputs = tf.nn.conv2d(images, filters, strides=1, padding=\"SAME\")\n",
    "\n",
    "plt.imshow(outputs[0, :, :, 1], cmap=\"gray\") # plot 1st image's 2nd feature map\n",
    "plt.axis(\"off\") # Not shown in the book\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_index in (0, 1):\n",
    "    for feature_map_index in (0, 1):\n",
    "        plt.subplot(2, 2, image_index * 2 + feature_map_index + 1)\n",
    "        plot_image(outputs[image_index, :, :, feature_map_index])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(images):\n",
    "    return images[150:220, 130:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image(crop(images[0, :, :, 0]))\n",
    "save_fig(\"china_original\", tight_layout=False)\n",
    "plt.show()\n",
    "\n",
    "for feature_map_index, filename in enumerate([\"china_vertical\", \"china_horizontal\"]):\n",
    "    plot_image(crop(outputs[0, :, :, feature_map_index]))\n",
    "    save_fig(filename, tight_layout=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image(filters[:, :, 0, 0])\n",
    "plt.show()\n",
    "plot_image(filters[:, :, 0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a 2D convolutional layer, using `keras.layers.Conv2D()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "conv = keras.layers.Conv2D(filters=2, kernel_size=7, strides=1,\n",
    "                           padding=\"SAME\", activation=\"relu\", input_shape=outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call this layer, passing it the two test images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_outputs = conv(images)\n",
    "conv_outputs.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a 4D tensor. The dimensions are: batch size, height, width, channels. The first dimension (batch size) is 2 since there are 2 input images. The next two dimensions are the height and width of the output feature maps: since `padding=\"SAME\"` and `strides=1`, the output feature maps have the same height and width as the input images (in this case, 427×640). Lastly, this convolutional layer has 2 filters, so the last dimension is 2: there are 2 output feature maps per input image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the filters are initialized randomly, they'll initially detect random patterns. Let's take a look at the 2 output features maps for each image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "for image_index in (0, 1):\n",
    "    for feature_map_index in (0, 1):\n",
    "        plt.subplot(2, 2, image_index * 2 + feature_map_index + 1)\n",
    "        plot_image(crop(conv_outputs[image_index, :, :, feature_map_index]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the filters were initialized randomly, the second filter happens to act like an edge detector. Randomly initialized filters often act this way, which is quite fortunate since detecting edges is quite useful in image processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want, we can set the filters to be the ones we manually defined earlier, and set the biases to zeros (in real life we will almost never need to set filters or biases manually, as the convolutional layer will just learn the appropriate filters and biases during training):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv.set_weights([filters, np.zeros(2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's call this layer again on the same two images, and let's check that the output feature maps do highlight vertical lines and horizontal lines, respectively (as earlier):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ab4ff65258c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconv_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mconv_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'conv' is not defined"
     ]
    }
   ],
   "source": [
    "conv_outputs = conv(images)\n",
    "conv_outputs.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "for image_index in (0, 1):\n",
    "    for feature_map_index in (0, 1):\n",
    "        plt.subplot(2, 2, image_index * 2 + feature_map_index + 1)\n",
    "        plot_image(crop(conv_outputs[image_index, :, :, feature_map_index]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VALID vs SAME padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_map_size(input_size, kernel_size, strides=1, padding=\"SAME\"):\n",
    "    if padding == \"SAME\":\n",
    "        return (input_size - 1) // strides + 1\n",
    "    else:\n",
    "        return (input_size - kernel_size) // strides + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_before_and_padded_size(input_size, kernel_size, strides=1):\n",
    "    fmap_size = feature_map_size(input_size, kernel_size, strides)\n",
    "    padded_size = max((fmap_size - 1) * strides + kernel_size, input_size)\n",
    "    pad_before = (padded_size - input_size) // 2\n",
    "    return pad_before, padded_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_same_padding(images, kernel_size, strides=1):\n",
    "    if kernel_size == 1:\n",
    "        return images.astype(np.float32)\n",
    "    batch_size, height, width, channels = images.shape\n",
    "    top_pad, padded_height = pad_before_and_padded_size(height, kernel_size, strides)\n",
    "    left_pad, padded_width  = pad_before_and_padded_size(width, kernel_size, strides)\n",
    "    padded_shape = [batch_size, padded_height, padded_width, channels]\n",
    "    padded_images = np.zeros(padded_shape, dtype=np.float32)\n",
    "    padded_images[:, top_pad:height+top_pad, left_pad:width+left_pad, :] = images\n",
    "    return padded_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `\"SAME\"` padding is equivalent to padding manually using `manual_same_padding()` then using `\"VALID\"` padding (confusingly, `\"VALID\"` padding means no padding at all):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 7\n",
    "strides = 2\n",
    "\n",
    "conv_valid = keras.layers.Conv2D(filters=1, kernel_size=kernel_size, strides=strides, padding=\"VALID\")\n",
    "conv_same = keras.layers.Conv2D(filters=1, kernel_size=kernel_size, strides=strides, padding=\"SAME\")\n",
    "\n",
    "valid_output = conv_valid(manual_same_padding(images, kernel_size, strides))\n",
    "\n",
    "# Need to call build() so conv_same's weights get created\n",
    "conv_same.build(tf.TensorShape(images.shape))\n",
    "\n",
    "# Copy the weights from conv_valid to conv_same\n",
    "conv_same.set_weights(conv_valid.get_weights())\n",
    "\n",
    "same_output = conv_same(images.astype(np.float32))\n",
    "\n",
    "assert np.allclose(valid_output.numpy(), same_output.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pooling layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pool = keras.layers.MaxPool2D(pool_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_images = np.array([crop(image) for image in images], dtype=np.float32)\n",
    "output = max_pool(cropped_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "gs = mpl.gridspec.GridSpec(nrows=1, ncols=2, width_ratios=[2, 1])\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.set_title(\"Input\", fontsize=14)\n",
    "ax1.imshow(cropped_images[0])  # plot the 1st image\n",
    "ax1.axis(\"off\")\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.set_title(\"Output\", fontsize=14)\n",
    "ax2.imshow(output[0])  # plot the output for the 1st image\n",
    "ax2.axis(\"off\")\n",
    "save_fig(\"china_max_pooling\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth-wise pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthMaxPool(keras.layers.Layer):\n",
    "    def __init__(self, pool_size, strides=None, padding=\"VALID\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        if strides is None:\n",
    "            strides = pool_size\n",
    "        self.pool_size = pool_size\n",
    "        self.strides = strides\n",
    "        self.padding = padding\n",
    "    def call(self, inputs):\n",
    "        return tf.nn.max_pool(inputs,\n",
    "                              ksize=(1, 1, 1, self.pool_size),\n",
    "                              strides=(1, 1, 1, self.pool_size),\n",
    "                              padding=self.padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_pool = DepthMaxPool(3)\n",
    "with tf.device(\"/cpu:0\"): # there is no GPU-kernel yet\n",
    "    depth_output = depth_pool(cropped_images)\n",
    "depth_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or just use a `Lambda` layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_pool = keras.layers.Lambda(lambda X: tf.nn.max_pool(\n",
    "    X, ksize=(1, 1, 1, 3), strides=(1, 1, 1, 3), padding=\"VALID\"))\n",
    "with tf.device(\"/cpu:0\"): # there is no GPU-kernel yet\n",
    "    depth_output = depth_pool(cropped_images)\n",
    "depth_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Input\", fontsize=14)\n",
    "plot_color_image(cropped_images[0])  # plot the 1st image\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Output\", fontsize=14)\n",
    "plot_image(depth_output[0, ..., 0])  # plot the output for the 1st image\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_pool = keras.layers.AvgPool2D(pool_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_avg = avg_pool(cropped_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "gs = mpl.gridspec.GridSpec(nrows=1, ncols=2, width_ratios=[2, 1])\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.set_title(\"Input\", fontsize=14)\n",
    "ax1.imshow(cropped_images[0])  # plot the 1st image\n",
    "ax1.axis(\"off\")\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.set_title(\"Output\", fontsize=14)\n",
    "ax2.imshow(output_avg[0])  # plot the output for the 1st image\n",
    "ax2.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Average Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_avg_pool = keras.layers.GlobalAvgPool2D()\n",
    "global_avg_pool(cropped_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_global_avg2 = keras.layers.Lambda(lambda X: tf.reduce_mean(X, axis=[1, 2]))\n",
    "output_global_avg2(cropped_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tackling Fashion MNIST With a CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-8.50525882e-03],\n",
       "        [-2.27560269e-02],\n",
       "        [-3.89675030e-02],\n",
       "        [-4.12281027e-02],\n",
       "        [-5.82534039e-02],\n",
       "        [-7.07237542e-02],\n",
       "        [-9.80052410e-02],\n",
       "        [-1.56204970e-01],\n",
       "        [-2.39456699e-01],\n",
       "        [-3.77741560e-01],\n",
       "        [-5.70428011e-01],\n",
       "        [-6.93861337e-01],\n",
       "        [-7.58059547e-01],\n",
       "        [-7.48321209e-01],\n",
       "        [-7.48453339e-01],\n",
       "        [-7.74419557e-01],\n",
       "        [-7.57517212e-01],\n",
       "        [-6.47070810e-01],\n",
       "        [-5.04646434e-01],\n",
       "        [-3.20677403e-01],\n",
       "        [-2.03425098e-01],\n",
       "        [-1.39628116e-01],\n",
       "        [-1.09701991e-01],\n",
       "        [-9.14734908e-02],\n",
       "        [-6.77413463e-02],\n",
       "        [-5.00065920e-02],\n",
       "        [-3.27325135e-02],\n",
       "        [-1.37618738e-02]],\n",
       "\n",
       "       [[-1.28614433e-02],\n",
       "        [-1.83370414e-02],\n",
       "        [-3.36992447e-02],\n",
       "        [-5.60637214e-02],\n",
       "        [-7.77566409e-02],\n",
       "        [-1.30692397e-01],\n",
       "        [-2.43712101e-01],\n",
       "        [-3.63297622e-01],\n",
       "        [-4.87224695e-01],\n",
       "        [-6.42904810e-01],\n",
       "        [-8.33877040e-01],\n",
       "        [-1.00207215e+00],\n",
       "        [-1.09061073e+00],\n",
       "        [-1.08165966e+00],\n",
       "        [-1.06535679e+00],\n",
       "        [-1.10644794e+00],\n",
       "        [-1.09002060e+00],\n",
       "        [-9.39738356e-01],\n",
       "        [-7.80206843e-01],\n",
       "        [-5.93941186e-01],\n",
       "        [-4.41232397e-01],\n",
       "        [-3.16415534e-01],\n",
       "        [-2.07545896e-01],\n",
       "        [-1.47916666e-01],\n",
       "        [-1.17966475e-01],\n",
       "        [-8.93286711e-02],\n",
       "        [-5.84899479e-02],\n",
       "        [-2.90112214e-02]],\n",
       "\n",
       "       [[-1.57760691e-02],\n",
       "        [-2.57730970e-02],\n",
       "        [-4.55254447e-02],\n",
       "        [-7.27447121e-02],\n",
       "        [-1.28551299e-01],\n",
       "        [-2.68558043e-01],\n",
       "        [-4.02733827e-01],\n",
       "        [-5.01763861e-01],\n",
       "        [-6.29380923e-01],\n",
       "        [-8.22354351e-01],\n",
       "        [-9.94525915e-01],\n",
       "        [-1.07042517e+00],\n",
       "        [-1.10077263e+00],\n",
       "        [-1.10005581e+00],\n",
       "        [-1.09213047e+00],\n",
       "        [-1.12111065e+00],\n",
       "        [-1.11718767e+00],\n",
       "        [-1.08198805e+00],\n",
       "        [-9.77343741e-01],\n",
       "        [-7.77020813e-01],\n",
       "        [-5.93248598e-01],\n",
       "        [-4.82048003e-01],\n",
       "        [-3.71965735e-01],\n",
       "        [-2.39619437e-01],\n",
       "        [-1.68923016e-01],\n",
       "        [-1.28807739e-01],\n",
       "        [-8.67338702e-02],\n",
       "        [-5.07535112e-02]],\n",
       "\n",
       "       [[-2.50818941e-02],\n",
       "        [-4.02066476e-02],\n",
       "        [-6.55495379e-02],\n",
       "        [-1.03087712e-01],\n",
       "        [-2.14381984e-01],\n",
       "        [-3.69864817e-01],\n",
       "        [-5.05805420e-01],\n",
       "        [-6.38148308e-01],\n",
       "        [-7.63100239e-01],\n",
       "        [-9.33389392e-01],\n",
       "        [-1.04969970e+00],\n",
       "        [-1.09513806e+00],\n",
       "        [-1.09523074e+00],\n",
       "        [-1.10400370e+00],\n",
       "        [-1.11197852e+00],\n",
       "        [-9.96945589e-01],\n",
       "        [-3.61471173e-01],\n",
       "        [-1.14154823e+00],\n",
       "        [-1.09078641e+00],\n",
       "        [-9.31243415e-01],\n",
       "        [-7.09774771e-01],\n",
       "        [-6.18720696e-01],\n",
       "        [-4.86613280e-01],\n",
       "        [-3.48390258e-01],\n",
       "        [-2.33471236e-01],\n",
       "        [-1.34988396e-01],\n",
       "        [-6.25890828e-02],\n",
       "        [-7.12048024e-02]],\n",
       "\n",
       "       [[-3.81346616e-02],\n",
       "        [-6.11484527e-02],\n",
       "        [-9.02680146e-02],\n",
       "        [-1.50055005e-01],\n",
       "        [-2.88032429e-01],\n",
       "        [-4.54069094e-01],\n",
       "        [-6.09802236e-01],\n",
       "        [-7.19682464e-01],\n",
       "        [-8.19596656e-01],\n",
       "        [-9.79983256e-01],\n",
       "        [-1.07710556e+00],\n",
       "        [-1.11082575e+00],\n",
       "        [-1.08428073e+00],\n",
       "        [-1.12589109e+00],\n",
       "        [-7.66005691e-01],\n",
       "        [ 2.89914815e-01],\n",
       "        [ 1.92479176e-01],\n",
       "        [-5.13130533e-01],\n",
       "        [-5.68040579e-01],\n",
       "        [-1.03714329e+00],\n",
       "        [-8.56429519e-01],\n",
       "        [-7.39879149e-01],\n",
       "        [-5.90193028e-01],\n",
       "        [-3.93815864e-01],\n",
       "        [-2.24181826e-01],\n",
       "        [-2.27462904e-01],\n",
       "        [-1.62348818e-01],\n",
       "        [ 2.31671519e-01]],\n",
       "\n",
       "       [[-5.14329247e-02],\n",
       "        [-8.41233056e-02],\n",
       "        [-1.22155132e-01],\n",
       "        [-1.99337074e-01],\n",
       "        [-3.48073465e-01],\n",
       "        [-5.31851492e-01],\n",
       "        [-6.73258345e-01],\n",
       "        [-7.70910610e-01],\n",
       "        [-8.67229248e-01],\n",
       "        [-1.02422627e+00],\n",
       "        [-1.10562472e+00],\n",
       "        [-1.13078960e+00],\n",
       "        [-1.06772764e+00],\n",
       "        [-1.15782519e+00],\n",
       "        [-9.01837808e-02],\n",
       "        [ 1.01027653e+00],\n",
       "        [ 7.00264306e-01],\n",
       "        [ 2.26429679e-01],\n",
       "        [ 3.61553999e-01],\n",
       "        [ 2.44850405e-01],\n",
       "        [-6.87550691e-01],\n",
       "        [-8.29950633e-01],\n",
       "        [-7.03979136e-01],\n",
       "        [-5.32606104e-01],\n",
       "        [-3.87886833e-01],\n",
       "        [-2.75035076e-02],\n",
       "        [ 9.79951373e-02],\n",
       "        [-1.23930021e-01]],\n",
       "\n",
       "       [[-6.62911043e-02],\n",
       "        [-1.06628268e-01],\n",
       "        [-1.53991026e-01],\n",
       "        [-2.44919204e-01],\n",
       "        [-3.99244220e-01],\n",
       "        [-5.88372466e-01],\n",
       "        [-7.19012703e-01],\n",
       "        [-8.12638451e-01],\n",
       "        [-8.97292984e-01],\n",
       "        [-1.05069842e+00],\n",
       "        [-1.12825749e+00],\n",
       "        [-1.15454402e+00],\n",
       "        [-1.15929749e+00],\n",
       "        [-1.19471382e+00],\n",
       "        [ 4.55672074e-01],\n",
       "        [ 1.33058336e+00],\n",
       "        [ 1.00599535e+00],\n",
       "        [ 6.72070415e-01],\n",
       "        [-1.14270476e-01],\n",
       "        [ 5.40004186e-01],\n",
       "        [ 7.40438595e-01],\n",
       "        [ 2.82508836e-01],\n",
       "        [-6.02925908e-02],\n",
       "        [-3.20674002e-01],\n",
       "        [ 6.95159320e-01],\n",
       "        [ 2.01278704e+00],\n",
       "        [ 1.59490858e+00],\n",
       "        [ 8.87314943e-01]],\n",
       "\n",
       "       [[-8.48425735e-02],\n",
       "        [-1.29727018e-01],\n",
       "        [-1.84206512e-01],\n",
       "        [-2.84040736e-01],\n",
       "        [-4.40811292e-01],\n",
       "        [-6.29932702e-01],\n",
       "        [-7.52908190e-01],\n",
       "        [-8.44434567e-01],\n",
       "        [-9.22231488e-01],\n",
       "        [-1.06344806e+00],\n",
       "        [-1.14585437e+00],\n",
       "        [-1.16816670e+00],\n",
       "        [-1.18846911e+00],\n",
       "        [-4.68145481e-01],\n",
       "        [ 1.02328201e+00],\n",
       "        [ 1.13915666e+00],\n",
       "        [ 1.08479695e+00],\n",
       "        [ 1.06898721e+00],\n",
       "        [ 1.09546502e+00],\n",
       "        [ 5.55712289e-01],\n",
       "        [ 2.74852497e-01],\n",
       "        [ 3.08535731e-01],\n",
       "        [ 4.78450729e-01],\n",
       "        [ 1.07119692e+00],\n",
       "        [ 1.39337368e+00],\n",
       "        [ 9.71167518e-01],\n",
       "        [ 3.57199857e+00],\n",
       "        [ 3.80888619e+00]],\n",
       "\n",
       "       [[-9.62265516e-02],\n",
       "        [-1.48908346e-01],\n",
       "        [-2.10231047e-01],\n",
       "        [-3.13745084e-01],\n",
       "        [-4.70928378e-01],\n",
       "        [-6.60139992e-01],\n",
       "        [-7.74491671e-01],\n",
       "        [-8.65561301e-01],\n",
       "        [-9.39036314e-01],\n",
       "        [-1.05869190e+00],\n",
       "        [-1.14820132e+00],\n",
       "        [-1.19366079e+00],\n",
       "        [-1.22322060e+00],\n",
       "        [ 9.69694608e-01],\n",
       "        [ 1.34043127e+00],\n",
       "        [ 1.19777896e+00],\n",
       "        [ 1.22060002e+00],\n",
       "        [ 1.18309785e+00],\n",
       "        [ 1.14035984e+00],\n",
       "        [ 1.18757901e+00],\n",
       "        [ 1.12718205e+00],\n",
       "        [ 1.18744284e+00],\n",
       "        [ 8.28952620e-01],\n",
       "        [ 6.99633845e-01],\n",
       "        [ 9.70622413e-01],\n",
       "        [ 2.31961487e+00],\n",
       "        [ 4.20260878e+00],\n",
       "        [-1.85973544e-01]],\n",
       "\n",
       "       [[-1.06071174e-01],\n",
       "        [-1.65331318e-01],\n",
       "        [-2.27957617e-01],\n",
       "        [-3.34138440e-01],\n",
       "        [-4.92392679e-01],\n",
       "        [-6.79675111e-01],\n",
       "        [-7.86830310e-01],\n",
       "        [-8.76619679e-01],\n",
       "        [-9.51978867e-01],\n",
       "        [-1.07230529e+00],\n",
       "        [-1.17852504e+00],\n",
       "        [-1.23709892e+00],\n",
       "        [-1.27237551e+00],\n",
       "        [ 7.43205272e-01],\n",
       "        [ 1.24584247e+00],\n",
       "        [ 9.43289467e-01],\n",
       "        [ 1.03718897e+00],\n",
       "        [ 1.12128689e+00],\n",
       "        [ 1.23777261e+00],\n",
       "        [ 1.19687474e+00],\n",
       "        [ 1.15295973e+00],\n",
       "        [ 1.21671129e+00],\n",
       "        [ 1.39199977e+00],\n",
       "        [ 1.65262197e+00],\n",
       "        [ 2.04718443e+00],\n",
       "        [ 2.68036187e+00],\n",
       "        [ 2.67983266e+00],\n",
       "        [-2.14460551e-01]],\n",
       "\n",
       "       [[-1.16385483e-01],\n",
       "        [-1.79047522e-01],\n",
       "        [-2.36255151e-01],\n",
       "        [-3.38893188e-01],\n",
       "        [-5.02954476e-01],\n",
       "        [-6.85130270e-01],\n",
       "        [-7.85643430e-01],\n",
       "        [-8.75507415e-01],\n",
       "        [-9.56858398e-01],\n",
       "        [-1.07624011e+00],\n",
       "        [-1.21980130e+00],\n",
       "        [-1.30657323e+00],\n",
       "        [-1.39830086e+00],\n",
       "        [ 7.92025909e-01],\n",
       "        [ 1.21600384e+00],\n",
       "        [ 9.11381544e-01],\n",
       "        [ 8.63223389e-01],\n",
       "        [ 6.70721596e-01],\n",
       "        [ 5.07751940e-01],\n",
       "        [ 9.95009668e-01],\n",
       "        [ 9.70298782e-01],\n",
       "        [ 1.06898685e+00],\n",
       "        [ 1.22298732e+00],\n",
       "        [ 1.58501179e+00],\n",
       "        [ 1.87532629e+00],\n",
       "        [ 2.37994495e+00],\n",
       "        [ 2.72979384e+00],\n",
       "        [-2.40534890e-01]],\n",
       "\n",
       "       [[-1.29936027e-01],\n",
       "        [-1.91258211e-01],\n",
       "        [-2.37881280e-01],\n",
       "        [-3.34239782e-01],\n",
       "        [-4.98037641e-01],\n",
       "        [-6.76323946e-01],\n",
       "        [-7.74514201e-01],\n",
       "        [-8.71096224e-01],\n",
       "        [-9.74138171e-01],\n",
       "        [-1.10616662e+00],\n",
       "        [-1.30189034e+00],\n",
       "        [-1.47455989e+00],\n",
       "        [-1.45844299e+00],\n",
       "        [ 1.05779626e+00],\n",
       "        [ 1.08504346e+00],\n",
       "        [ 8.20168838e-01],\n",
       "        [ 9.13442742e-01],\n",
       "        [ 5.53293814e-01],\n",
       "        [ 3.07248506e-01],\n",
       "        [ 1.13096793e+00],\n",
       "        [ 9.17197431e-01],\n",
       "        [ 1.11492353e+00],\n",
       "        [ 1.29906996e+00],\n",
       "        [ 1.39091307e+00],\n",
       "        [ 1.86232251e+00],\n",
       "        [ 1.67371390e+00],\n",
       "        [ 2.45536568e+00],\n",
       "        [ 1.73639770e+00]],\n",
       "\n",
       "       [[-1.44213424e-01],\n",
       "        [-2.02748633e-01],\n",
       "        [-2.45382531e-01],\n",
       "        [-3.37715091e-01],\n",
       "        [-5.03204945e-01],\n",
       "        [-6.71979570e-01],\n",
       "        [-7.73705306e-01],\n",
       "        [-8.86721042e-01],\n",
       "        [-1.03583546e+00],\n",
       "        [-1.24519619e+00],\n",
       "        [-1.44758965e+00],\n",
       "        [-1.64460915e+00],\n",
       "        [-5.12419970e-01],\n",
       "        [ 1.36073437e+00],\n",
       "        [ 1.08381257e+00],\n",
       "        [ 9.19503038e-01],\n",
       "        [ 8.92181596e-01],\n",
       "        [ 6.81570460e-01],\n",
       "        [ 6.40453011e-01],\n",
       "        [ 1.02036455e+00],\n",
       "        [ 9.51168725e-01],\n",
       "        [ 1.01280860e+00],\n",
       "        [ 1.22194254e+00],\n",
       "        [ 1.40455269e+00],\n",
       "        [ 2.00961461e+00],\n",
       "        [ 7.03162605e-01],\n",
       "        [ 1.63328358e+00],\n",
       "        [ 1.68701665e+00]],\n",
       "\n",
       "       [[-1.53066016e-01],\n",
       "        [-2.17771346e-01],\n",
       "        [-2.63297952e-01],\n",
       "        [-3.59277045e-01],\n",
       "        [-5.36232482e-01],\n",
       "        [-7.10632229e-01],\n",
       "        [-8.26198361e-01],\n",
       "        [-9.51847272e-01],\n",
       "        [-1.12294795e+00],\n",
       "        [-1.30767082e+00],\n",
       "        [-1.62588228e+00],\n",
       "        [-1.76929194e+00],\n",
       "        [-1.17473500e+00],\n",
       "        [ 1.22898285e+00],\n",
       "        [ 1.13718451e+00],\n",
       "        [ 1.04968748e+00],\n",
       "        [ 1.01729104e+00],\n",
       "        [ 1.19442498e+00],\n",
       "        [ 1.08261002e+00],\n",
       "        [ 9.25663428e-01],\n",
       "        [ 9.86783861e-01],\n",
       "        [ 1.09118011e+00],\n",
       "        [ 1.31129015e+00],\n",
       "        [ 1.33071133e+00],\n",
       "        [ 1.65486323e+00],\n",
       "        [ 1.71543047e+00],\n",
       "        [ 5.58670454e-01],\n",
       "        [-3.35111449e-01]],\n",
       "\n",
       "       [[-1.82077915e-01],\n",
       "        [-2.72352917e-01],\n",
       "        [-3.03980446e-01],\n",
       "        [-3.57929736e-01],\n",
       "        [-5.35629364e-01],\n",
       "        [-7.16495175e-01],\n",
       "        [-8.76874819e-01],\n",
       "        [-1.00197512e+00],\n",
       "        [-1.16748507e+00],\n",
       "        [-1.43587807e+00],\n",
       "        [-1.74049864e+00],\n",
       "        [-1.92538651e+00],\n",
       "        [ 1.18234577e+00],\n",
       "        [ 1.08287898e+00],\n",
       "        [ 9.85227801e-01],\n",
       "        [ 9.52882919e-01],\n",
       "        [ 9.18476562e-01],\n",
       "        [ 8.92570526e-01],\n",
       "        [ 9.35792840e-01],\n",
       "        [ 1.03415675e+00],\n",
       "        [ 9.97953017e-01],\n",
       "        [ 1.09763960e+00],\n",
       "        [ 1.25557593e+00],\n",
       "        [ 1.29548877e+00],\n",
       "        [ 1.65244790e+00],\n",
       "        [ 2.23689821e+00],\n",
       "        [ 3.18088384e-01],\n",
       "        [-3.65033916e-01]],\n",
       "\n",
       "       [[-2.49883890e-01],\n",
       "        [-2.96553393e-01],\n",
       "        [-3.99141371e-01],\n",
       "        [-4.95713620e-01],\n",
       "        [-6.89413199e-01],\n",
       "        [-8.56746939e-01],\n",
       "        [-9.42602801e-01],\n",
       "        [-1.03638943e+00],\n",
       "        [-1.22386366e+00],\n",
       "        [-8.09250640e-01],\n",
       "        [-3.23447311e-03],\n",
       "        [ 7.36317316e-01],\n",
       "        [ 1.04327182e+00],\n",
       "        [ 8.14598671e-01],\n",
       "        [ 9.17097575e-01],\n",
       "        [ 9.17580696e-01],\n",
       "        [ 8.44887757e-01],\n",
       "        [ 7.20506836e-01],\n",
       "        [ 7.64554585e-01],\n",
       "        [ 9.88638425e-01],\n",
       "        [ 1.10522924e+00],\n",
       "        [ 1.09039286e+00],\n",
       "        [ 1.13825802e+00],\n",
       "        [ 1.27824103e+00],\n",
       "        [ 1.68491444e+00],\n",
       "        [ 2.08407839e+00],\n",
       "        [ 1.21866762e+00],\n",
       "        [-3.91647050e-01]],\n",
       "\n",
       "       [[-3.10259791e-01],\n",
       "        [-4.03767716e-01],\n",
       "        [-4.47619392e-01],\n",
       "        [-5.48055691e-01],\n",
       "        [-5.27651044e-01],\n",
       "        [-4.20502613e-01],\n",
       "        [-1.13920789e-01],\n",
       "        [ 7.69597987e-02],\n",
       "        [ 8.77790323e-01],\n",
       "        [ 1.13312369e+00],\n",
       "        [ 9.52005820e-01],\n",
       "        [ 9.60725984e-01],\n",
       "        [ 8.70250756e-01],\n",
       "        [ 1.04422125e+00],\n",
       "        [ 7.24190102e-01],\n",
       "        [ 6.86987578e-01],\n",
       "        [ 7.28698086e-01],\n",
       "        [ 1.03936668e+00],\n",
       "        [ 9.41951154e-01],\n",
       "        [ 1.17808634e+00],\n",
       "        [ 5.92938569e-01],\n",
       "        [ 7.33194952e-01],\n",
       "        [ 1.47107108e+00],\n",
       "        [ 1.62893089e+00],\n",
       "        [ 1.77390103e+00],\n",
       "        [ 2.02562195e+00],\n",
       "        [ 1.85836701e+00],\n",
       "        [-4.11652001e-01]],\n",
       "\n",
       "       [[-3.23036450e-01],\n",
       "        [ 5.33147686e-01],\n",
       "        [ 2.46493707e+00],\n",
       "        [ 2.30602563e+00],\n",
       "        [ 1.90527965e+00],\n",
       "        [ 1.48642350e+00],\n",
       "        [ 1.36716774e+00],\n",
       "        [ 1.16746563e+00],\n",
       "        [ 1.02063698e+00],\n",
       "        [ 9.28953760e-01],\n",
       "        [ 7.71188827e-01],\n",
       "        [ 7.56940388e-01],\n",
       "        [ 6.19662900e-01],\n",
       "        [ 1.54341171e-01],\n",
       "        [ 1.29506278e+00],\n",
       "        [ 5.35551024e-01],\n",
       "        [ 6.64531808e-01],\n",
       "        [ 9.51245114e-01],\n",
       "        [ 1.40016035e+00],\n",
       "        [ 1.46278886e+00],\n",
       "        [ 1.17244901e+00],\n",
       "        [ 1.27651272e+00],\n",
       "        [ 1.21148134e+00],\n",
       "        [ 1.27327457e+00],\n",
       "        [ 1.68031658e+00],\n",
       "        [ 2.04600902e+00],\n",
       "        [ 2.32711473e+00],\n",
       "        [-4.08194576e-01]],\n",
       "\n",
       "       [[-1.80612125e-01],\n",
       "        [ 3.06748124e+00],\n",
       "        [ 3.04920968e+00],\n",
       "        [ 2.49046051e+00],\n",
       "        [ 1.85157467e+00],\n",
       "        [ 1.38020453e+00],\n",
       "        [ 1.22827793e+00],\n",
       "        [ 1.25631950e+00],\n",
       "        [ 1.02430595e+00],\n",
       "        [ 8.14376182e-01],\n",
       "        [ 7.37755872e-01],\n",
       "        [ 9.20870115e-01],\n",
       "        [ 1.19320674e+00],\n",
       "        [-8.27036218e-01],\n",
       "        [ 1.30120565e-01],\n",
       "        [ 1.40570373e+00],\n",
       "        [ 1.01794003e+00],\n",
       "        [ 9.36816174e-01],\n",
       "        [ 5.25630662e-01],\n",
       "        [ 2.94833346e-01],\n",
       "        [ 9.07694266e-01],\n",
       "        [ 1.08615092e+00],\n",
       "        [ 1.07487689e+00],\n",
       "        [ 1.29601752e+00],\n",
       "        [ 1.76496569e+00],\n",
       "        [ 2.08734603e+00],\n",
       "        [ 2.19428981e+00],\n",
       "        [-3.97056024e-01]],\n",
       "\n",
       "       [[ 3.64077410e+00],\n",
       "        [ 3.69762128e+00],\n",
       "        [ 2.62115834e+00],\n",
       "        [ 2.30317774e+00],\n",
       "        [ 1.86787874e+00],\n",
       "        [ 1.59488710e+00],\n",
       "        [ 1.44342772e+00],\n",
       "        [ 1.52240079e+00],\n",
       "        [ 1.53976939e+00],\n",
       "        [ 1.01725585e+00],\n",
       "        [ 6.07922982e-01],\n",
       "        [ 8.52694960e-01],\n",
       "        [ 8.74181269e-01],\n",
       "        [ 1.25422649e+00],\n",
       "        [-8.64549300e-01],\n",
       "        [-9.49436778e-01],\n",
       "        [-6.42232186e-01],\n",
       "        [-4.46140825e-01],\n",
       "        [ 3.20382909e-01],\n",
       "        [ 1.09459788e+00],\n",
       "        [ 1.31152477e+00],\n",
       "        [ 1.20450232e+00],\n",
       "        [ 1.25714419e+00],\n",
       "        [ 1.50128552e+00],\n",
       "        [ 1.84928119e+00],\n",
       "        [ 2.14023570e+00],\n",
       "        [ 2.43674578e+00],\n",
       "        [ 6.05755798e-01]],\n",
       "\n",
       "       [[ 2.70376058e+00],\n",
       "        [ 3.23449776e+00],\n",
       "        [ 2.89617455e+00],\n",
       "        [ 2.25367944e+00],\n",
       "        [ 1.54361905e+00],\n",
       "        [ 1.35252206e+00],\n",
       "        [ 1.27876489e+00],\n",
       "        [ 1.46334606e+00],\n",
       "        [ 1.16726357e+00],\n",
       "        [ 6.17330607e-01],\n",
       "        [ 6.70630884e-01],\n",
       "        [ 7.43046830e-01],\n",
       "        [ 6.45359066e-01],\n",
       "        [ 9.21238126e-01],\n",
       "        [ 1.27129574e+00],\n",
       "        [ 6.73301935e-01],\n",
       "        [ 1.00774125e+00],\n",
       "        [ 1.26368731e+00],\n",
       "        [ 1.25293044e+00],\n",
       "        [ 1.19686558e+00],\n",
       "        [ 1.36427391e+00],\n",
       "        [ 1.26928229e+00],\n",
       "        [ 1.24210816e+00],\n",
       "        [ 1.57334407e+00],\n",
       "        [ 1.93974776e+00],\n",
       "        [ 2.29423187e+00],\n",
       "        [ 2.76871395e+00],\n",
       "        [ 2.15521596e+00]],\n",
       "\n",
       "       [[ 1.75977426e+00],\n",
       "        [ 3.40005020e+00],\n",
       "        [ 2.55887797e+00],\n",
       "        [ 2.18504855e+00],\n",
       "        [ 1.82515989e+00],\n",
       "        [ 1.29489140e+00],\n",
       "        [ 1.04581415e+00],\n",
       "        [ 1.13130360e+00],\n",
       "        [ 9.42457684e-01],\n",
       "        [ 7.34464451e-01],\n",
       "        [ 7.60742055e-01],\n",
       "        [ 8.55047023e-01],\n",
       "        [ 9.22410308e-01],\n",
       "        [ 1.03251161e+00],\n",
       "        [ 1.06374199e+00],\n",
       "        [ 1.19400540e+00],\n",
       "        [ 1.01393518e+00],\n",
       "        [ 9.55147439e-01],\n",
       "        [ 8.55198372e-01],\n",
       "        [ 1.07711997e+00],\n",
       "        [ 1.09579453e+00],\n",
       "        [ 1.03576475e+00],\n",
       "        [ 9.77406179e-01],\n",
       "        [ 1.10173971e+00],\n",
       "        [ 1.58201808e+00],\n",
       "        [ 2.27185058e+00],\n",
       "        [ 2.68725799e+00],\n",
       "        [ 4.19637390e+00]],\n",
       "\n",
       "       [[-2.18063391e-01],\n",
       "        [ 2.19715884e+00],\n",
       "        [ 3.47922350e+00],\n",
       "        [ 2.34611758e+00],\n",
       "        [ 1.51192354e+00],\n",
       "        [ 1.08749450e+00],\n",
       "        [ 1.10326572e+00],\n",
       "        [ 1.28936118e+00],\n",
       "        [ 1.10257809e+00],\n",
       "        [ 9.88726619e-01],\n",
       "        [ 9.37590038e-01],\n",
       "        [ 8.10160569e-01],\n",
       "        [ 8.69157649e-01],\n",
       "        [ 9.51843765e-01],\n",
       "        [ 9.03422224e-01],\n",
       "        [ 8.08811909e-01],\n",
       "        [ 7.32080392e-01],\n",
       "        [ 7.46672669e-01],\n",
       "        [ 8.97157885e-01],\n",
       "        [ 9.87880895e-01],\n",
       "        [ 1.33286685e+00],\n",
       "        [ 1.27919933e+00],\n",
       "        [ 1.06465928e+00],\n",
       "        [ 1.03181967e+00],\n",
       "        [ 1.56220524e+00],\n",
       "        [ 2.13588156e+00],\n",
       "        [ 3.17861158e+00],\n",
       "        [ 3.94905912e+00]],\n",
       "\n",
       "       [[-1.84482578e-01],\n",
       "        [-2.63445726e-01],\n",
       "        [ 1.11358614e+00],\n",
       "        [ 2.51850820e+00],\n",
       "        [ 2.05703288e+00],\n",
       "        [ 1.40184900e+00],\n",
       "        [ 1.12146771e+00],\n",
       "        [ 1.13090952e+00],\n",
       "        [ 8.72508503e-01],\n",
       "        [ 7.52424787e-01],\n",
       "        [ 6.93507513e-01],\n",
       "        [ 6.60212307e-01],\n",
       "        [ 6.85700849e-01],\n",
       "        [ 7.78058299e-01],\n",
       "        [ 9.02567995e-01],\n",
       "        [ 9.11325372e-01],\n",
       "        [ 9.18876616e-01],\n",
       "        [ 1.01282075e+00],\n",
       "        [ 1.13704491e+00],\n",
       "        [ 1.26862099e+00],\n",
       "        [ 1.60123238e+00],\n",
       "        [ 1.37747419e+00],\n",
       "        [ 1.30474131e+00],\n",
       "        [ 1.60253263e+00],\n",
       "        [ 2.09421866e+00],\n",
       "        [ 3.08685342e+00],\n",
       "        [ 3.00050043e+00],\n",
       "        [-1.87338928e-01]],\n",
       "\n",
       "       [[ 5.20663743e-03],\n",
       "        [-2.11450370e-01],\n",
       "        [-2.83699105e-01],\n",
       "        [-4.06186990e-01],\n",
       "        [ 2.94512615e-01],\n",
       "        [ 1.62227676e+00],\n",
       "        [ 1.78180046e+00],\n",
       "        [ 2.04190859e+00],\n",
       "        [ 1.67994941e+00],\n",
       "        [ 1.50390246e+00],\n",
       "        [ 1.44651236e+00],\n",
       "        [ 1.32984922e+00],\n",
       "        [ 1.35345987e+00],\n",
       "        [ 1.21145189e+00],\n",
       "        [ 1.28322168e+00],\n",
       "        [ 9.53735515e-01],\n",
       "        [ 8.64974433e-01],\n",
       "        [ 7.70243187e-01],\n",
       "        [ 9.25055598e-01],\n",
       "        [ 1.07167337e+00],\n",
       "        [ 1.38734312e+00],\n",
       "        [ 1.39521926e+00],\n",
       "        [ 1.17956488e+00],\n",
       "        [ 1.42318881e+00],\n",
       "        [ 9.59123346e-01],\n",
       "        [ 7.01744264e-01],\n",
       "        [-2.61124465e-01],\n",
       "        [-1.50209368e-01]],\n",
       "\n",
       "       [[-1.00021040e-01],\n",
       "        [-1.61644591e-01],\n",
       "        [-2.33688010e-01],\n",
       "        [-3.57268925e-01],\n",
       "        [-5.31964856e-01],\n",
       "        [-6.18408709e-01],\n",
       "        [-6.61311521e-01],\n",
       "        [-1.79995122e-01],\n",
       "        [-1.28687755e-01],\n",
       "        [-4.68223911e-01],\n",
       "        [-2.81668268e-01],\n",
       "        [-6.79871498e-01],\n",
       "        [-7.27468112e-01],\n",
       "        [-1.00295902e+00],\n",
       "        [-9.24703584e-01],\n",
       "        [-9.70978550e-01],\n",
       "        [-1.02480703e+00],\n",
       "        [-1.01010304e+00],\n",
       "        [-9.35749705e-01],\n",
       "        [-8.13149622e-01],\n",
       "        [-6.80343533e-01],\n",
       "        [-6.41723639e-01],\n",
       "        [-6.36149144e-01],\n",
       "        [-5.81691531e-01],\n",
       "        [-4.52467212e-01],\n",
       "        [-2.96817953e-01],\n",
       "        [-2.08504125e-01],\n",
       "        [-1.11065735e-01]],\n",
       "\n",
       "       [[-5.65180838e-02],\n",
       "        [-1.04847038e-01],\n",
       "        [-1.77919992e-01],\n",
       "        [-3.04919985e-01],\n",
       "        [-4.73777810e-01],\n",
       "        [-5.57585423e-01],\n",
       "        [-5.91817522e-01],\n",
       "        [-5.86487252e-01],\n",
       "        [-6.77477165e-01],\n",
       "        [-8.07991888e-01],\n",
       "        [-9.16149110e-01],\n",
       "        [-9.73281684e-01],\n",
       "        [-9.59438317e-01],\n",
       "        [-8.79929133e-01],\n",
       "        [-8.13364046e-01],\n",
       "        [-8.59647284e-01],\n",
       "        [-9.16024963e-01],\n",
       "        [-9.06862970e-01],\n",
       "        [-8.32001788e-01],\n",
       "        [-7.01920344e-01],\n",
       "        [-5.82017078e-01],\n",
       "        [-5.69941621e-01],\n",
       "        [-5.73362377e-01],\n",
       "        [-5.19671720e-01],\n",
       "        [-3.98093113e-01],\n",
       "        [-2.42097888e-01],\n",
       "        [-1.54730338e-01],\n",
       "        [-7.55138703e-02]],\n",
       "\n",
       "       [[-2.51620282e-02],\n",
       "        [-4.94793557e-02],\n",
       "        [-1.02531246e-01],\n",
       "        [-2.08662989e-01],\n",
       "        [-3.40149570e-01],\n",
       "        [-4.24968605e-01],\n",
       "        [-4.35514279e-01],\n",
       "        [-4.05185448e-01],\n",
       "        [-4.56149388e-01],\n",
       "        [-5.54007566e-01],\n",
       "        [-6.71849892e-01],\n",
       "        [-7.52669058e-01],\n",
       "        [-7.53855196e-01],\n",
       "        [-6.88444053e-01],\n",
       "        [-6.43190495e-01],\n",
       "        [-6.77677272e-01],\n",
       "        [-7.30196588e-01],\n",
       "        [-7.03079465e-01],\n",
       "        [-6.01215335e-01],\n",
       "        [-4.74965608e-01],\n",
       "        [-3.94622858e-01],\n",
       "        [-4.05504388e-01],\n",
       "        [-4.40912537e-01],\n",
       "        [-3.96689004e-01],\n",
       "        [-2.88576670e-01],\n",
       "        [-1.56697027e-01],\n",
       "        [-8.95454958e-02],\n",
       "        [-3.38411152e-02]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\n",
    "\n",
    "X_mean = X_train.mean(axis=0, keepdims=True)\n",
    "X_std = X_train.std(axis=0, keepdims=True) + 1e-7\n",
    "X_train = (X_train - X_mean) / X_std\n",
    "X_valid = (X_valid - X_mean) / X_std\n",
    "X_test = (X_test - X_mean) / X_std\n",
    "\n",
    "X_train = X_train[..., np.newaxis]\n",
    "X_valid = X_valid[..., np.newaxis]\n",
    "X_test = X_test[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "DefaultConv2D = partial(keras.layers.Conv2D,\n",
    "                        kernel_size=3, activation='relu', padding=\"SAME\")\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    DefaultConv2D(filters=64, kernel_size=7, input_shape=[28, 28, 1]),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    DefaultConv2D(filters=128),\n",
    "    DefaultConv2D(filters=128),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    DefaultConv2D(filters=256),\n",
    "    DefaultConv2D(filters=256),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(units=128, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(units=64, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(units=10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\n",
    "score = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:10] # pretend we have new images\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet-34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "DefaultConv2D = partial(keras.layers.Conv2D, kernel_size=3, strides=1,\n",
    "                        padding=\"SAME\", use_bias=False)\n",
    "\n",
    "class ResidualUnit(keras.layers.Layer):\n",
    "    def __init__(self, filters, strides=1, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.activation = keras.activations.get(activation)\n",
    "        self.main_layers = [\n",
    "            DefaultConv2D(filters, strides=strides),\n",
    "            keras.layers.BatchNormalization(),\n",
    "            self.activation,\n",
    "            DefaultConv2D(filters),\n",
    "            keras.layers.BatchNormalization()]\n",
    "        self.skip_layers = []\n",
    "        if strides > 1:\n",
    "            self.skip_layers = [\n",
    "                DefaultConv2D(filters, kernel_size=1, strides=strides),\n",
    "                keras.layers.BatchNormalization()]\n",
    "\n",
    "    def call(self, inputs):\n",
    "        Z = inputs\n",
    "        for layer in self.main_layers:\n",
    "            Z = layer(Z)\n",
    "        skip_Z = inputs\n",
    "        for layer in self.skip_layers:\n",
    "            skip_Z = layer(skip_Z)\n",
    "        return self.activation(Z + skip_Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(DefaultConv2D(64, kernel_size=7, strides=2,\n",
    "                        input_shape=[224, 224, 3]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Activation(\"relu\"))\n",
    "model.add(keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"SAME\"))\n",
    "prev_filters = 64\n",
    "for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n",
    "    strides = 1 if filters == prev_filters else 2\n",
    "    model.add(ResidualUnit(filters, strides=strides))\n",
    "    prev_filters = filters\n",
    "model.add(keras.layers.GlobalAvgPool2D())\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 112, 112, 64)      9408      \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 112, 112, 64)     256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation (Activation)     (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 56, 56, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " residual_unit (ResidualUnit  (None, 56, 56, 64)       74240     \n",
      " )                                                               \n",
      "                                                                 \n",
      " residual_unit_1 (ResidualUn  (None, 56, 56, 64)       74240     \n",
      " it)                                                             \n",
      "                                                                 \n",
      " residual_unit_2 (ResidualUn  (None, 56, 56, 64)       74240     \n",
      " it)                                                             \n",
      "                                                                 \n",
      " residual_unit_3 (ResidualUn  (None, 28, 28, 128)      230912    \n",
      " it)                                                             \n",
      "                                                                 \n",
      " residual_unit_4 (ResidualUn  (None, 28, 28, 128)      295936    \n",
      " it)                                                             \n",
      "                                                                 \n",
      " residual_unit_5 (ResidualUn  (None, 28, 28, 128)      295936    \n",
      " it)                                                             \n",
      "                                                                 \n",
      " residual_unit_6 (ResidualUn  (None, 28, 28, 128)      295936    \n",
      " it)                                                             \n",
      "                                                                 \n",
      " residual_unit_7 (ResidualUn  (None, 14, 14, 256)      920576    \n",
      " it)                                                             \n",
      "                                                                 \n",
      " residual_unit_8 (ResidualUn  (None, 14, 14, 256)      1181696   \n",
      " it)                                                             \n",
      "                                                                 \n",
      " residual_unit_9 (ResidualUn  (None, 14, 14, 256)      1181696   \n",
      " it)                                                             \n",
      "                                                                 \n",
      " residual_unit_10 (ResidualU  (None, 14, 14, 256)      1181696   \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_11 (ResidualU  (None, 14, 14, 256)      1181696   \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_12 (ResidualU  (None, 14, 14, 256)      1181696   \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_13 (ResidualU  (None, 7, 7, 512)        3676160   \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_14 (ResidualU  (None, 7, 7, 512)        4722688   \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " residual_unit_15 (ResidualU  (None, 7, 7, 512)        4722688   \n",
      " nit)                                                            \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 512)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,306,826\n",
      "Trainable params: 21,289,802\n",
      "Non-trainable params: 17,024\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.applications.resnet50.ResNet50(weights=\"imagenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_resized = tf.image.resize(images, [224, 224])\n",
    "plot_color_image(images_resized[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_resized = tf.image.resize_with_pad(images, 224, 224, antialias=True)\n",
    "plot_color_image(images_resized[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_resized = tf.image.resize_with_crop_or_pad(images, 224, 224)\n",
    "plot_color_image(images_resized[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "china_box = [0, 0.03, 1, 0.68]\n",
    "flower_box = [0.19, 0.26, 0.86, 0.7]\n",
    "images_resized = tf.image.crop_and_resize(images, [china_box, flower_box], [0, 1], [224, 224])\n",
    "plot_color_image(images_resized[0])\n",
    "plt.show()\n",
    "plot_color_image(images_resized[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.applications.resnet50.preprocess_input(images_resized * 255)\n",
    "Y_proba = model.predict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_proba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_K = keras.applications.resnet50.decode_predictions(Y_proba, top=3)\n",
    "for image_index in range(len(images)):\n",
    "    print(\"Image #{}\".format(image_index))\n",
    "    for class_id, name, y_proba in top_K[image_index]:\n",
    "        print(\"  {} - {:12s} {:.2f}%\".format(class_id, name, y_proba * 100))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained Models for Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "dataset, info = tfds.load(\"tf_flowers\", as_supervised=True, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info.splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info.splits[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = info.features[\"label\"].names\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = info.features[\"label\"].num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = info.splits[\"train\"].num_examples\n",
    "dataset_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning:** TFDS's split API has evolved since the book was published. The [new split API](https://www.tensorflow.org/datasets/splits) (called S3) is much simpler to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_raw, valid_set_raw, train_set_raw = tfds.load(\n",
    "    \"tf_flowers\",\n",
    "    split=[\"train[:10%]\", \"train[10%:25%]\", \"train[25%:]\"],\n",
    "    as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "index = 0\n",
    "for image, label in train_set_raw.take(9):\n",
    "    index += 1\n",
    "    plt.subplot(3, 3, index)\n",
    "    plt.imshow(image)\n",
    "    plt.title(\"Class: {}\".format(class_names[label]))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image, label):\n",
    "    resized_image = tf.image.resize(image, [224, 224])\n",
    "    final_image = keras.applications.xception.preprocess_input(resized_image)\n",
    "    return final_image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly fancier preprocessing (but you could add much more data augmentation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def central_crop(image):\n",
    "    shape = tf.shape(image)\n",
    "    min_dim = tf.reduce_min([shape[0], shape[1]])\n",
    "    top_crop = (shape[0] - min_dim) // 4\n",
    "    bottom_crop = shape[0] - top_crop\n",
    "    left_crop = (shape[1] - min_dim) // 4\n",
    "    right_crop = shape[1] - left_crop\n",
    "    return image[top_crop:bottom_crop, left_crop:right_crop]\n",
    "\n",
    "def random_crop(image):\n",
    "    shape = tf.shape(image)\n",
    "    min_dim = tf.reduce_min([shape[0], shape[1]]) * 90 // 100\n",
    "    return tf.image.random_crop(image, [min_dim, min_dim, 3])\n",
    "\n",
    "def preprocess(image, label, randomize=False):\n",
    "    if randomize:\n",
    "        cropped_image = random_crop(image)\n",
    "        cropped_image = tf.image.random_flip_left_right(cropped_image)\n",
    "    else:\n",
    "        cropped_image = central_crop(image)\n",
    "    resized_image = tf.image.resize(cropped_image, [224, 224])\n",
    "    final_image = keras.applications.xception.preprocess_input(resized_image)\n",
    "    return final_image, label\n",
    "\n",
    "batch_size = 32\n",
    "train_set = train_set_raw.shuffle(1000).repeat()\n",
    "train_set = train_set.map(partial(preprocess, randomize=True)).batch(batch_size).prefetch(1)\n",
    "valid_set = valid_set_raw.map(preprocess).batch(batch_size).prefetch(1)\n",
    "test_set = test_set_raw.map(preprocess).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "for X_batch, y_batch in train_set.take(1):\n",
    "    for index in range(9):\n",
    "        plt.subplot(3, 3, index + 1)\n",
    "        plt.imshow(X_batch[index] / 2 + 0.5)\n",
    "        plt.title(\"Class: {}\".format(class_names[y_batch[index]]))\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "for X_batch, y_batch in test_set.take(1):\n",
    "    for index in range(9):\n",
    "        plt.subplot(3, 3, index + 1)\n",
    "        plt.imshow(X_batch[index] / 2 + 0.5)\n",
    "        plt.title(\"Class: {}\".format(class_names[y_batch[index]]))\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.xception.Xception(weights=\"imagenet\",\n",
    "                                                  include_top=False)\n",
    "avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "output = keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
    "model = keras.models.Model(inputs=base_model.input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, layer in enumerate(base_model.layers):\n",
    "    print(index, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.2, momentum=0.9, decay=0.01)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set,\n",
    "                    steps_per_epoch=int(0.75 * dataset_size / batch_size),\n",
    "                    validation_data=valid_set,\n",
    "                    validation_steps=int(0.15 * dataset_size / batch_size),\n",
    "                    epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9,\n",
    "                                 nesterov=True, decay=0.001)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set,\n",
    "                    steps_per_epoch=int(0.75 * dataset_size / batch_size),\n",
    "                    validation_data=valid_set,\n",
    "                    validation_steps=int(0.15 * dataset_size / batch_size),\n",
    "                    epochs=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and Localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.xception.Xception(weights=\"imagenet\",\n",
    "                                                  include_top=False)\n",
    "avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "class_output = keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
    "loc_output = keras.layers.Dense(4)(avg)\n",
    "model = keras.models.Model(inputs=base_model.input,\n",
    "                           outputs=[class_output, loc_output])\n",
    "model.compile(loss=[\"sparse_categorical_crossentropy\", \"mse\"],\n",
    "              loss_weights=[0.8, 0.2], # depends on what you care most about\n",
    "              optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_random_bounding_boxes(images, labels):\n",
    "    fake_bboxes = tf.random.uniform([tf.shape(images)[0], 4])\n",
    "    return images, (labels, fake_bboxes)\n",
    "\n",
    "fake_train_set = train_set.take(5).repeat(2).map(add_random_bounding_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(fake_train_set, steps_per_epoch=5, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Average Precision (mAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximum_precisions(precisions):\n",
    "    return np.flip(np.maximum.accumulate(np.flip(precisions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recalls = np.linspace(0, 1, 11)\n",
    "\n",
    "precisions = [0.91, 0.94, 0.96, 0.94, 0.95, 0.92, 0.80, 0.60, 0.45, 0.20, 0.10]\n",
    "max_precisions = maximum_precisions(precisions)\n",
    "mAP = max_precisions.mean()\n",
    "plt.plot(recalls, precisions, \"ro--\", label=\"Precision\")\n",
    "plt.plot(recalls, max_precisions, \"bo-\", label=\"Max Precision\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.plot([0, 1], [mAP, mAP], \"g:\", linewidth=3, label=\"mAP\")\n",
    "plt.grid(True)\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.legend(loc=\"lower center\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transpose convolutions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "X = images_resized.numpy()\n",
    "\n",
    "conv_transpose = keras.layers.Conv2DTranspose(filters=5, kernel_size=3, strides=2, padding=\"VALID\")\n",
    "output = conv_transpose(X)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    return (X - tf.reduce_min(X)) / (tf.reduce_max(X) - tf.reduce_min(X))\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "gs = mpl.gridspec.GridSpec(nrows=1, ncols=2, width_ratios=[1, 2])\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.set_title(\"Input\", fontsize=14)\n",
    "ax1.imshow(X[0])  # plot the 1st image\n",
    "ax1.axis(\"off\")\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.set_title(\"Output\", fontsize=14)\n",
    "ax2.imshow(normalize(output[0, ..., :3]), interpolation=\"bicubic\")  # plot the output for the 1st image\n",
    "ax2.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upscale_images(images, stride, kernel_size):\n",
    "    batch_size, height, width, channels = images.shape\n",
    "    upscaled = np.zeros((batch_size,\n",
    "                         (height - 1) * stride + 2 * kernel_size - 1,\n",
    "                         (width - 1) * stride + 2 * kernel_size - 1,\n",
    "                         channels))\n",
    "    upscaled[:,\n",
    "             kernel_size - 1:(height - 1) * stride + kernel_size:stride,\n",
    "             kernel_size - 1:(width - 1) * stride + kernel_size:stride,\n",
    "             :] = images\n",
    "    return upscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upscaled = upscale_images(X, stride=2, kernel_size=3)\n",
    "weights, biases = conv_transpose.weights\n",
    "reversed_filters = np.flip(weights.numpy(), axis=[0, 1])\n",
    "reversed_filters = np.transpose(reversed_filters, [0, 1, 3, 2])\n",
    "manual_output = tf.nn.conv2d(upscaled, reversed_filters, strides=1, padding=\"VALID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    return (X - tf.reduce_min(X)) / (tf.reduce_max(X) - tf.reduce_min(X))\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "gs = mpl.gridspec.GridSpec(nrows=1, ncols=3, width_ratios=[1, 2, 2])\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.set_title(\"Input\", fontsize=14)\n",
    "ax1.imshow(X[0])  # plot the 1st image\n",
    "ax1.axis(\"off\")\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.set_title(\"Upscaled\", fontsize=14)\n",
    "ax2.imshow(upscaled[0], interpolation=\"bicubic\")\n",
    "ax2.axis(\"off\")\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "ax3.set_title(\"Output\", fontsize=14)\n",
    "ax3.imshow(normalize(manual_output[0, ..., :3]), interpolation=\"bicubic\")  # plot the output for the 1st image\n",
    "ax3.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(output, manual_output.numpy(), atol=1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. to 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See appendix A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. High Accuracy CNN for MNIST\n",
    "_Exercise: Build your own CNN from scratch and try to achieve the highest possible accuracy on MNIST._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following model uses 2 convolutional layers, followed by 1 pooling layer, then dropout 25%, then a dense layer, another dropout layer but with 50% dropout, and finally the output layer. It reaches about 99.2% accuracy on the test set. This places this model roughly in the top 20% in the [MNIST Kaggle competition](https://www.kaggle.com/c/digit-recognizer/) (if we ignore the models with an accuracy greater than 99.79% which were most likely trained on the test set, as explained by Chris Deotte in [this post](https://www.kaggle.com/c/digit-recognizer/discussion/61480)). Can you do better? To reach 99.5 to 99.7% accuracy on the test set, you need to add image augmentation, batch norm, use a learning schedule such as 1-cycle, and possibly create an ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "X_train_full = X_train_full / 255.\n",
    "X_test = X_test / 255.\n",
    "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\n",
    "\n",
    "X_train = X_train[..., np.newaxis]\n",
    "X_valid = X_valid[..., np.newaxis]\n",
    "X_test = X_test[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(32, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "    keras.layers.MaxPool2D(),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dropout(0.25),\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.  Use transfer learning for large image classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise: Use transfer learning for large image classification, going through these steps:_\n",
    "\n",
    "* _Create a training set containing at least 100 images per class. For example, you could classify your own pictures based on the location (beach, mountain, city, etc.), or alternatively you can use an existing dataset (e.g., from TensorFlow Datasets)._\n",
    "* _Split it into a training set, a validation set, and a test set._\n",
    "* _Build the input pipeline, including the appropriate preprocessing operations, and optionally add data augmentation._\n",
    "* _Fine-tune a pretrained model on this dataset._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the Flowers example above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.\n",
    "_Exercise: Go through TensorFlow's [Style Transfer tutorial](https://homl.info/styletuto). It is a fun way to generate art using Deep Learning._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply open the Colab and follow its instructions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "678a026ccb0dc0d017e80ee3aeab54fd4a4d81f807e2651e2fcff2dd84b6f2fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
